# **TimeStream - A Versioned Data Lakehouse with Atomic ETL Pipelines**  

ğŸš€ **Data Versioning | Time Travel | Git-Like Branching | Atomic ETL**  

**TimeStream** is a **modern data lakehouse** designed for **data versioning, reproducibility, and atomic ETL pipelines**. It combines **Git-like data management** with **structured storage** to enable **branching, merging, and time travel** for analytics and AI applications.

With **TimeStream**, data engineers can work in isolated development branches, validate transformations, and merge updates seamlessly into productionâ€”ensuring **data consistency, efficiency, and scalability**.

---

## **Key Features**  
âœ… **Git-Inspired Data Management** â€“ Use branches and commits for tracking data changes over time.  
âœ… **Atomic ETL Pipelines** â€“ Guarantee **data consistency** by merging only validated changes.  
âœ… **Time Travel & Auditing** â€“ Query **historical snapshots** using commit hashes or timestamps.  
âœ… **Efficient Storage** â€“ Symbolic branching **reduces data duplication** and optimizes storage.  
âœ… **Dockerized Deployment** â€“ Fully **containerized stack**, easy to set up and run.  

---

## **Tech Stack**
| Technology  | Purpose |
|-------------|---------|
| **MinIO**   | S3-compatible object storage for data lake storage. |
| **Apache Iceberg** | Table format enabling time travel, snapshotting, and schema evolution. |
| **Nessie**  | Git-like catalog for data versioning, branching, and merging. |
| **Apache Spark** | Distributed computing engine for ETL and transformations. |
| **Jupyter Notebooks** | Interactive exploration and visualization of datasets. |

---

## **Dataset**
We use the **NYC Taxi Trip Data**, a real-world dataset from the [New York City Taxi and Limousine Commission](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page).  

### **Dataset Overview**
- **Source:** Public dataset containing **trip details**, including pickup/drop-off locations, timestamps, fares, and passenger counts.
- **Format:** **Parquet**
- **Size:** **~1GB per month of data**  
- **Use Case:** Used for demonstrating **ETL processes**, **branch-based transformations**, and **time travel queries**.

---

## **Workflow: Versioned ETL Pipeline**
TimeStream follows a structured, **branch-based** workflow for data processing, similar to a Git workflow:

### **1ï¸âƒ£ Raw Data Ingestion (`raw` branch)**
- Ingest raw data into **MinIO** object storage.
- Store **initial unprocessed data** in Apache Iceberg under a separate branch.

### **2ï¸âƒ£ Data Transformation (`dev` branch)**
- Run transformations using **Apache Spark**.
- Perform **data cleaning**, **aggregations**, and **format conversions**.
- Store **intermediate results** in a separate development branch.

### **3ï¸âƒ£ Validation & Quality Checks (`dev` branch)**
- Run automated **validation checks** to ensure data quality.
- Verify **schema correctness, missing values, and logical consistency**.

### **4ï¸âƒ£ Promotion to Production (`main` branch)**
- Merge **validated** changes from `dev` into `main`.
- Ensure **atomic updates**, guaranteeing a consistent view of the data for consumers.

### **5ï¸âƒ£ Time Travel & Auditing (Commit Hashes & Tags)**
- Retrieve **historical snapshots** using **Nessie commit hashes**.
- Query **previous states** of the data for **auditing and debugging**.

---

## **Getting Started**
Follow these **step-by-step instructions** to set up and run TimeStream.

### **1ï¸âƒ£ Clone the Repository**
```sh
git clone https://github.com/muratkars/TimeStream.git
cd TimeStream
```

### **2ï¸âƒ£ Start the Environment**
Use Docker Compose to start the services:
```sh
docker-compose up -d
```
This will spin up MinIO, Nessie, Apache Spark, Iceberg REST, and Jupyter Notebooks.

### **3ï¸âƒ£ Explore the Data**
Open Jupyter Notebooks and explore the data:
```sh
jupyter notebook
```

### **4ï¸âƒ£ Verify Services**
Run the following command to confirm everything is running:
```sh
docker ps
```
Ensure the following services are running:
- minio
- nessie
- spark-iceberg
- iceberg-rest
- mc
- jupyter

### **5ï¸âƒ£ Run ETL Pipelines**  
Step 1: Ingest Data

Download and store raw data in MinIO:

```sh
python etl/ingest.py
```

Step 2: Transform Data

Process and clean data using Apache Spark: 

```sh
python etl/transform.py
```

Step 3: Validate & Merge Data
Validate the cleaned dataset before merging to main:    

```sh
python etl/validate.py
```

### **6ï¸âƒ£ Explore Data in Jupyter**   
Launch Jupyter Notebook to query and explore datasets:

```sh
Open http://localhost:8888 and navigate to `notebooks/exploration.ipynb`
```

### **7ï¸âƒ£ Query Data in Jupyter** 

```sh
df = spark.read.format("iceberg").load("nessie.timestream.cleaned_trips")
df.show(5)

```

To retrieve previous versions:

```sh
snapshot_id = "your_snapshot_id_here"
df_old = spark.read.format("iceberg").option("snapshot-id", snapshot_id).load("nessie.timestream.cleaned_trips")
df_old.show(5)
```

## **Troubleshooting**

â— MinIO Access Issues

- Check if MinIO is running:

    ```sh
    docker logs minio
    ```
- Manually log in to MinIO:

   - Open http://localhost:9001
   - Login: admin
   - Password: password 

â— Jupyter Notebook Not Accessible

- Restart the Jupyter service:  

    ```sh
    docker-compose restart jupyter
    ```
â— Spark Job Fails

- Check Spark logs:

    ```sh
    docker logs spark-iceberg
    ```

## **Folder Structure**

```
TimeStream/
â”‚â”€â”€ docker-compose.yml
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md
â”‚â”€â”€ LICENSE
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ ingest.py        # Raw data ingestion
â”‚   â”œâ”€â”€ transform.py     # Data transformation using Spark
â”‚   â”œâ”€â”€ validate.py      # Data validation before merging
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ exploration.ipynb  # Jupyter Notebook for interactive queries
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ nessie_config.json
â”‚   â”œâ”€â”€ iceberg_config.json
```

## **License**
This project is licensed under the Apache License 2.0 - see the LICENSE file for details.  

## **Next Steps**
ğŸ”¹ Expand dataset integration by including multi-year taxi trip data
ğŸ”¹ Implement real-time streaming ETL with Kafka
ğŸ”¹ Improve data governance with role-based access control
ğŸ”¹ Add more data sources and transformations.
ğŸ”¹ Implement more advanced data validation and quality checks.
ğŸ”¹ Add more interactive visualizations and dashboards.
ğŸ”¹ Add more advanced data versioning and branching strategies.

## **Acknowledgments**
Thanks to the contributors and maintainers of the technologies used in this project.

